{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "from src import display\n",
    "from src import data_transformer as dt\n",
    "from src.evaluation import jaccard, non_max_suppression, PredBoundingBox, MAP\n",
    "from src.augmentations import RandomHorizontalFlip, RandomContrast\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = \"./data/VOCdevkit/VOC2012/JPEGImages/\"\n",
    "train_json_path = \"./data/VOCdevkit/VOC2012/cocoformatJson/voc_2012_train.json\"\n",
    "val_json_path = \"./data/VOCdevkit/VOC2012/cocoformatJson/voc_2012_val.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 224\n",
    "\n",
    "seed = 42\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "id_cat, temp_list = dt.load_pascal(train_json_path)\n",
    "data_list += temp_list\n",
    "_, temp_list = dt.load_pascal(val_json_path)\n",
    "data_list += temp_list\n",
    "\n",
    "random.seed(seed)\n",
    "random.shuffle(data_list)\n",
    "\n",
    "# print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = dt.rescale_bounding_boxes(data_list, target_size)\n",
    "data_list = dt.convert_to_center(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = data_list[3]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_str = IMG_PATH + example.filename\n",
    "img = display.read_img(img_str, target_size)\n",
    "\n",
    "img = display.draw_boxes(img, example.bounding_boxes)\n",
    "img = display.draw_text(img, example.classnames, example.bounding_boxes)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_grid = [7, 4, 2, 1] # Number of grid-elements per dimension\n",
    "anchor_zooms = [0.7, 1.0, 1.3] # How much bigger/smaller each default box will be (percentage)\n",
    "anchor_ratios = [(1.0, 1.0), (1.0, 0.5), (0.5, 1.0)] # Ratio between (width, height)\n",
    "\n",
    "anchor_scales = [(anc*h, anc*w) for anc in anchor_zooms for (h,w) in anchor_ratios]\n",
    "anchor_offsets = [1/(2*o) for o in anchor_grid]\n",
    "num_boxes = len(anchor_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_x = [np.repeat(np.linspace(ao, 1-ao, ag), ag) for (ao, ag) in zip(anchor_offsets, anchor_grid)]\n",
    "anchor_x = np.concatenate(anchor_x)\n",
    "\n",
    "anchor_y = [np.tile(np.linspace(ao, 1-ao, ag), ag) for ao,ag in zip(anchor_offsets, anchor_grid)]\n",
    "anchor_y = np.concatenate(anchor_y)\n",
    "\n",
    "anchor_centers = np.repeat(np.stack([anchor_x, anchor_y], axis=1), num_boxes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_sizes  = [np.array([[w/ag, h/ag] for _ in range(ag**2) for w, h in anchor_scales])\n",
    "                 for ag in anchor_grid]\n",
    "anchor_sizes = np.concatenate(anchor_sizes)\n",
    "\n",
    "anchors = np.concatenate([anchor_centers, anchor_sizes], axis=1)\n",
    "anchors = torch.from_numpy(anchors).float()\n",
    "anchors = anchors.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PascalData(Dataset):\n",
    "    def __init__(self, data_list_, target_size_=target_size, path_=IMG_PATH, p=0.5, train_mode=False):\n",
    "        self.target_size = target_size_\n",
    "        self.path = path_\n",
    "        self.data_list = data_list_\n",
    "        self.p = p\n",
    "\n",
    "        self.mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
    "        self.std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
    "\n",
    "        self.train_mode = train_mode\n",
    "        self.flip = RandomHorizontalFlip(p)\n",
    "        self.contrast = RandomContrast(p)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data_list[idx]\n",
    "\n",
    "        img_str = self.path + item.filename\n",
    "        img = display.read_img(img_str, self.target_size)\n",
    "        img = img / 255.0\n",
    "\n",
    "        gt = np.vstack(item.bounding_boxes)\n",
    "\n",
    "        if self.train_mode:\n",
    "            img = self.contrast(img)\n",
    "            img, gt = self.flip(img, gt)\n",
    "\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        img = (img - self.mean) / self.std\n",
    "        img = torch.from_numpy(img).float().to(device)\n",
    "\n",
    "        #gt = gt / target_size\n",
    "        gt = torch.from_numpy(gt).float() / self.target_size\n",
    "        gt = gt[None, :, :].to(device)\n",
    "\n",
    "        c = np.array(item.class_id) + 1\n",
    "        c = torch.from_numpy(np.array(item.class_id)) + 1\n",
    "        c = c[None, :].to(device)\n",
    "\n",
    "        return (img, gt, c)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Writing custom collector function since the Dataset class returns both tensors and lists.\n",
    "    \"\"\"\n",
    "\n",
    "    x = [b[0] for b in batch]\n",
    "    x = torch.stack(x, dim=0)\n",
    "    gt = [b[1] for b in batch]\n",
    "    c = [b[2] for b in batch]\n",
    "    return (x, gt, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_size = 0.9\n",
    "\n",
    "split_idx = int(train_size * len(data_list))\n",
    "\n",
    "train_dataset = PascalData(data_list[0:split_idx], train_mode=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = PascalData(data_list[split_idx:])\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, p, stride=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout2d(p=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x        \n",
    "\n",
    "class OutputConv(nn.Module):\n",
    "    def __init__(self, in_channels, num_boxes):\n",
    "        super().__init__()\n",
    "        self.num_boxes = num_boxes\n",
    "        self.conv_1 = nn.Conv2d(in_channels, (len(id_cat) + 1) * self.num_boxes, kernel_size=3, padding=1)\n",
    "        self.conv_2 = nn.Conv2d(in_channels, 4 * self.num_boxes, kernel_size=3, padding=1)\n",
    "\n",
    "    def flatten_conv(self, x):\n",
    "        samples, channels, _, _ = x.size()\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        return x.view(samples, -1, int(channels / self.num_boxes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [class predictions, box coordinates]\n",
    "        return [self.flatten_conv(self.conv_1(x)), \n",
    "                self.flatten_conv(self.conv_2(x))]\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_boxes, p):\n",
    "        super().__init__()\n",
    "\n",
    "        pretrained_model = list(models.resnet34(pretrained='imagenet').children())[:-2]\n",
    "        self.backbone = nn.Sequential(*pretrained_model)\n",
    "        self.backbone_dropout = nn.Dropout2d(p=p[0])\n",
    "\n",
    "        self.std_conv = nn.ModuleList([\n",
    "            StandardConv(512, 256, p[1], stride=1),\n",
    "            StandardConv(256, 256, p[1], stride=1),\n",
    "            StandardConv(256, 256, p[1]),\n",
    "            StandardConv(256, 256, p[1]),\n",
    "            StandardConv(256, 256, p[1])\n",
    "        ])\n",
    "\n",
    "        self.out_conv = nn.ModuleList([\n",
    "            OutputConv(256, num_boxes),\n",
    "            OutputConv(256, num_boxes),\n",
    "            OutputConv(256, num_boxes),\n",
    "            OutputConv(256, num_boxes)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.backbone_dropout(x)\n",
    "\n",
    "        x = self.std_conv[0](x)\n",
    "        x = self.std_conv[1](x)\n",
    "        output_class_0, output_bb_0 = self.out_conv[0](x)\n",
    "\n",
    "        x = self.std_conv[2](x)\n",
    "        output_class_1, output_bb_1 = self.out_conv[1](x)\n",
    "\n",
    "        x = self.std_conv[3](x)\n",
    "        output_class_2, output_bb_2 = self.out_conv[2](x)\n",
    "\n",
    "        x = self.std_conv[4](x)\n",
    "        output_class_3, output_bb_3 = self.out_conv[3](x)\n",
    "\n",
    "        # Class, bounding box\n",
    "        return [torch.cat([output_class_0, output_class_1, output_class_2, output_class_3], dim=1),\n",
    "                torch.cat([output_bb_0, output_bb_1, output_bb_2, output_bb_3], dim=1)\n",
    "                ]\n",
    "\n",
    "    def change_freezing(self, mode=False):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = mode\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode)\n",
    "\n",
    "        # Avoid updating BN stats\n",
    "        for m in self.backbone.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overlap(bb_true_i, anchors, jaccard_overlap):\n",
    "    jaccard_tensor = jaccard(anchors, bb_true_i)\n",
    "    _, max_overlap = torch.max(jaccard_tensor, dim=0)\n",
    "\n",
    "    overlap_list = []    \n",
    "    for i in range(len(bb_true_i)):\n",
    "        threshold_overlap = (jaccard_tensor[:, i] > jaccard_overlap).nonzero()\n",
    "\n",
    "        if len(threshold_overlap) > 0:\n",
    "            threshold_overlap = threshold_overlap[:, 0]\n",
    "            overlap = torch.cat([max_overlap[i].view(1), threshold_overlap])\n",
    "            overlap = torch.unique(overlap)     \n",
    "        else:\n",
    "            overlap = max_overlap[i].view(1)\n",
    "        overlap_list.append(overlap)\n",
    "    return overlap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, eps=1e-10):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        p = torch.sigmoid(input)\n",
    "        pt = p * target.float() + (1.0 - p) * (1 - target).float()\n",
    "        alpha_t = (1.0 - self.alpha) * target.float() + self.alpha * (1 - target).float()\n",
    "        loss = - 1.0 * torch.pow((1 - pt), self.gamma) * torch.log(pt + self.eps)\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDLoss(nn.Module):\n",
    "    def __init__(self, loc_factor, anchors, jaccard_overlap, **kwargs):\n",
    "        super().__init__()\n",
    "        self.fl = FocalLoss(**kwargs)\n",
    "        self.loc_factor = loc_factor\n",
    "        self.jaccard_overlap = jaccard_overlap\n",
    "        self.anchors = anchors\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot_encoding(labels, num_classes):\n",
    "        return torch.eye(num_classes)[labels.cpu()]\n",
    "\n",
    "    @staticmethod\n",
    "    def loc_transformation(x, anchors, overlap_indicies):\n",
    "        # Doing location transformations according to SSD paper\n",
    "        return torch.cat([(x[:, 0:1] - anchors[overlap_indicies, 0:1]) / anchors[overlap_indicies, 2:3],\n",
    "                          (x[:, 1:2] - anchors[overlap_indicies, 1:2]) / anchors[overlap_indicies, 3:4],\n",
    "                          torch.log((x[:, 2:3] / anchors[overlap_indicies, 2:3])),\n",
    "                          torch.log((x[:, 3:4] / anchors[overlap_indicies, 3:4]))\n",
    "                         ], dim=1)\n",
    "\n",
    "    def forward(self, class_hat, bb_hat, class_true, bb_true):        \n",
    "        loc_loss = 0.0\n",
    "        class_loss = 0.0\n",
    "\n",
    "        for i in range(len(class_true)):  # Batch level\n",
    "            class_hat_i = class_hat[i, :, :]\n",
    "            bb_true_i = bb_true[i]\n",
    "            class_true_i = class_true[i]\n",
    "            class_target = torch.zeros(class_hat_i.shape[0]).long().to(device)\n",
    "\n",
    "            overlap_list = find_overlap(bb_true_i.squeeze(0), self.anchors, self.jaccard_overlap)\n",
    "\n",
    "            temp_loc_loss = 0.0\n",
    "            for j in range(len(overlap_list)):  # BB level\n",
    "                overlap = overlap_list[j]\n",
    "                class_target[overlap] = class_true_i[0, j]\n",
    "\n",
    "                input_ = bb_hat[i, overlap, :]\n",
    "                target_ = SSDLoss.loc_transformation(bb_true_i[0, j, :].expand((len(overlap), 4)), self.anchors, overlap)\n",
    "\n",
    "                temp_loc_loss += F.smooth_l1_loss(input=input_, target=target_, reduction=\"sum\") / len(overlap)\n",
    "            loc_loss += temp_loc_loss / class_true_i.shape[1]\n",
    "\n",
    "            class_target = SSDLoss.one_hot_encoding(class_target, len(id_cat) + 1).float().to(device)\n",
    "            class_loss += self.fl(class_hat_i, class_target) / class_true_i.shape[1]\n",
    "\n",
    "        loc_loss = loc_loss / len(class_true)\n",
    "        class_loss = class_loss / len(class_true)\n",
    "        loss = class_loss + loc_loss * self.loc_factor\n",
    "\n",
    "        return loss, loc_loss, class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_factor = 10.0\n",
    "jaccard_overlap = 0.6\n",
    "\n",
    "loss = SSDLoss(loc_factor=loc_factor, anchors=anchors, jaccard_overlap=jaccard_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [0.2, 0.5]\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "model = Model(num_boxes=num_boxes, p=p).to(device)\n",
    "model.change_freezing(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "lr = 1e-3\n",
    "wd = 1e-2\n",
    "\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_threshold = 0.5\n",
    "map_eval = MAP(model, val_dataset, jaccard_threshold, anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "for epoch in range(n_epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_loc_loss, train_class_loss  = 0.0, 0.0, 0.0\n",
    "    for _, (x, bb_true, class_true) in enumerate(train_loader):\n",
    "        model.zero_grad()\n",
    "        class_hat, bb_hat = model(x)\n",
    "\n",
    "        batch_loss, batch_loc, batch_class = loss(class_hat, bb_hat, class_true, bb_true)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += batch_loss.detach().cpu().numpy()\n",
    "        train_class_loss += batch_class.detach().cpu().numpy()\n",
    "        train_loc_loss += batch_loc.detach().cpu().numpy()\n",
    "\n",
    "    train_loss =  np.round(train_loss / len(train_loader), 6)\n",
    "    train_loc_loss =  np.round(train_loc_loss / len(train_loader), 6)\n",
    "    train_class_loss =  np.round(train_class_loss / len(train_loader), 6)\n",
    "\n",
    "    val_loss, val_loc_loss, val_class_loss = 0.0, 0.0, 0.0\n",
    "    for _, (x, bb_true, class_true) in enumerate(val_loader):\n",
    "        class_hat, bb_hat = model(x)\n",
    "\n",
    "        batch_loss, batch_loc, batch_class = loss(class_hat, bb_hat, class_true, bb_true)\n",
    "        val_loss += batch_loss.detach().cpu().numpy()\n",
    "        val_loc_loss += batch_loc.detach().cpu().numpy()\n",
    "        val_class_loss += batch_class.detach().cpu().numpy()\n",
    "\n",
    "    val_loss = np.round(val_loss / len(val_loader), 6)\n",
    "    val_loc_loss = np.round(val_loc_loss / len(val_loader), 6)\n",
    "    val_class_loss = np.round(val_class_loss / len(val_loader), 6)\n",
    "\n",
    "    _, mAP = map_eval()\n",
    "    mAP = np.round(mAP, 6)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"----- Epoch {epoch} -----\")\n",
    "        print(f\"Train loss: {train_loss}\")\n",
    "        print(f\"Train loc loss: {train_loc_loss}\")\n",
    "        print(f\"Train class loss: {train_class_loss}\")\n",
    "        print(f\"Val loss: {val_loss}\")\n",
    "        print(f\"Val loc loss: {val_loc_loss}\")\n",
    "        print(f\"Val class loss: {val_class_loss}\")\n",
    "        print(f\"Val mAP: {mAP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_threshold = 0.9\n",
    "\n",
    "i = 1\n",
    "(x, bb_true, class_true) = val_dataset[i]\n",
    "img_file = val_dataset.data_list[i].filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "class_hat, bb_hat = model(x.unsqueeze(0))\n",
    "\n",
    "bb_hat = dt.invert_transformation(bb_hat.squeeze(0), anchors)\n",
    "bb_hat = bb_hat * target_size\n",
    "\n",
    "class_hat = class_hat.sigmoid().squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering with < since we want low probability of background\n",
    "bb_hat = bb_hat[class_hat[:,0] < background_threshold, :]\n",
    "bb_hat = bb_hat.detach().cpu().numpy()\n",
    "class_hat = class_hat[class_hat[:,0] < background_threshold, :]\n",
    "class_preds = class_hat[:, 1:]\n",
    "\n",
    "print(class_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(class_preds) > 0:\n",
    "    prob, class_id = class_preds.max(1)\n",
    "    prob = prob.detach().cpu().numpy()\n",
    "    class_id = class_id.detach().cpu().numpy()\n",
    "else:\n",
    "    prob = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bb = [PredBoundingBox(probability=prob[j],\n",
    "                             class_id=class_id[j],\n",
    "                             classname=id_cat[class_id[j]],\n",
    "                             bounding_box=[bb_hat[j, 0], \n",
    "                                           bb_hat[j, 1], \n",
    "                                           bb_hat[j, 2], \n",
    "                                           bb_hat[j, 3]])\n",
    "             for j in range(len(prob))]\n",
    "\n",
    "output_bb = sorted(output_bb, key = lambda x: x.probability, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bb = non_max_suppression(output_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = data_list[split_idx:][i]\n",
    "img_str = IMG_PATH + img_file\n",
    "f, axs = plt.subplots(1, 2, figsize=(12, 12))\n",
    "\n",
    "img_pred = display.read_img(img_str, target_size)\n",
    "img_pred = display.draw_boxes(img_pred, [bb.bounding_box for bb in filtered_bb])\n",
    "img_pred = display.draw_text(img_pred, [bb.classname for bb in filtered_bb], [bb.bounding_box for bb in filtered_bb])\n",
    "axs[0].imshow(img_pred)\n",
    "axs[0].set_title(\"Predicted\")\n",
    "\n",
    "img_gt = display.read_img(img_str, target_size)\n",
    "img_gt = display.draw_boxes(img_gt, example.bounding_boxes)\n",
    "img_gt = display.draw_text(img_gt, example.classnames, example.bounding_boxes)\n",
    "axs[1].imshow(img_gt)\n",
    "axs[1].set_title(\"Ground-truth\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('ailab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b7f3b3627a51ec38d876704f1cf446c4e10a62dff35bfc702a2cacf3788da608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
